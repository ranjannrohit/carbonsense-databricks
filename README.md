# ğŸŒ **CarbonSense**
## **Revealing the True Carbon Cost of Economic Growth using AI on Databricks**

---

## ğŸš€ **Project Summary**

**CarbonSense** is an end-to-end **Databricks Lakehouse + AI analytics solution** designed to measure **carbon inefficiency** across emission sources and countries.

Instead of only reporting absolute COâ‚‚ emissions, this project uses **machine learning** to learn the *expected* COâ‚‚ emissions based on economic and energy indicators and compares them with *actual* emissions to identify:

- âŒ Carbon-inefficient sectors and countries  
- âœ… Carbon-efficient best practices  
- ğŸ¯ Priority areas for climate intervention  

The project demonstrates how **Databricks, Delta Lake, ML, and SQL analytics** can be combined to build a **real-world decision support system** for climate intelligence.

---

## ğŸ¯ **Problem Statement**

Traditional climate analysis focuses on **absolute emissions**, which fails to account for economic scale.

### â“ The Core Question
> *Is a country or sector emitting more COâ‚‚ than it reasonably should, given its economic activity?*

### âœ… Solution Approach
CarbonSense introduces a new metric:


- **Positive Carbon Gap** â†’ Carbon-inefficient (problematic)
- **Negative Carbon Gap** â†’ Carbon-efficient (good performance)

This allows fair, data-driven comparison across sectors and countries.

---

## ğŸ¤– **Why AI is Required**

Rule-based thresholds cannot capture the complex relationship between:
- GDP
- Energy consumption
- Sector-level emissions

AI enables:
- Learning expected emissions dynamically
- Comparing performance beyond raw totals
- Turning predictions into **actionable insights**

---

## ğŸ—ï¸ **Architecture Overview**
### *(Databricks Lakehouse â€“ Medallion Architecture)*


---

### ğŸŸ¤ **Bronze Layer â€“ Raw Data**
- Source: OWID COâ‚‚ dataset
- Stored as Delta table


---

### âšª **Silver Layer â€“ Cleaned & Structured**
- Sector-level emissions
- Country & year mapping
- Economic and energy indicators


Predictions are **persisted back to Delta**, enabling downstream analytics.

---

## ğŸ¤– **Machine Learning Component**

- **ML Task:** Regression
- **Model Used:** Linear Regression (interpretable & explainable)
- **Features:**
  - GDP
  - Energy per capita
- **Target Variable:** Sector-level COâ‚‚ emissions

### ğŸ“Š **Model Evaluation**
- Train/Test Split: 80/20
- Metric Used: **RMSE**
- RMSE Achieved: **17829**

> *Note:* COâ‚‚ values are measured at large absolute scale; the objective is **relative deviation (carbon gap)**, not exact prediction accuracy.

---

## ğŸ”„ **Database â†” AI Workflow**

1. Raw data ingested into Delta tables
2. Features engineered in Gold layer
3. ML model trained on Gold data
4. Predictions written back to Delta
5. Databricks SQL dashboards consume prediction tables

This creates a **closed-loop Lakehouse + AI system**.

---

## ğŸ“Š **Analytics & Dashboards**
*(Built using Databricks SQL)*

### Key Insights Delivered:
- ğŸ“Œ KPI: Average Global Carbon Inefficiency
- ğŸ“Œ KPI: % of emissions exceeding AI-expected levels
- ğŸ“Œ KPI: Most carbon-inefficient sector
- ğŸ“Œ KPI: Most carbon-efficient sector
- ğŸ“ˆ Line Chart: Actual vs AI-Expected COâ‚‚ emissions
- ğŸ“Š Bar Chart: Carbon efficiency by emission source
- ğŸŒ Bar Chart: Top carbon-inefficient countries
- ğŸ¥§ Pie Chart: Sector-wise contribution to global COâ‚‚

All dashboards are powered directly from **Delta tables**, ensuring governance and consistency.

---

## ğŸŒ± **Business Impact & Use Cases**

### ğŸ‘¥ Who Can Use This?
- Government climate policy teams
- ESG & sustainability analysts
- Climate research organizations
- Energy & industrial planners

### ğŸ§  Decisions Enabled:
- Identify sectors requiring emission controls
- Benchmark efficient industries
- Prioritize countries for climate intervention
- Support data-driven climate policy

---

## ğŸ› ï¸ **Tech Stack**

- Databricks Lakehouse
- Delta Lake
- PySpark
- Spark MLlib
- Databricks SQL
- MLflow
- GitHub

---

## â–¶ï¸ **How to Run the Project (High-Level)**

1. Upload raw dataset to Databricks
2. Execute Bronze â†’ Silver â†’ Gold notebooks
3. Train ML model and generate predictions
4. Build SQL dashboards from Gold tables

---

## ğŸ“š **Key Learnings**

- Designing an end-to-end Lakehouse architecture
- Applying ML for insight generation, not just prediction
- Using Databricks SQL for decision-grade analytics
- Converting data into real-world climate insights

---

## ğŸ§‘â€ğŸ’» **Author**

**Rohit Ranjan**  
Aspiring Data Analyst | Data & AI Enthusiast  

---

## ğŸ”— **Project Links**
- GitHub Repository: *(add link here)*
- LinkedIn Submission Post: *(add link after submission)*

---
